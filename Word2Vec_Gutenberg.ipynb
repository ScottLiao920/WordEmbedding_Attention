{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all sentences in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for sent in gutenberg.sents(file):\n",
    "#         sents.append(sent)\n",
    "# len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model using gensim for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(sents, size=100, window=5, min_count=1, workers=4)\n",
    "# model.save('word2vec_gensim.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary for all the words in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for word in gutenberg.words(file):\n",
    "#         words.append(word)\n",
    "# # words = list(set(words))\n",
    "# len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd = nltk.FreqDist(words)\n",
    "# vocab = sorted(fd, key=fd.get, reverse=True)\n",
    "# vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab', 'wb') as f:\n",
    "#     for i in range(len(vocab)):\n",
    "#         f.write('{} {}\\n'.format(vocab[i], i).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.json', 'w') as f:\n",
    "#     json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'window_size': 5,\n",
    "    'num_epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'batch_size': 512,\n",
    "    'num_heads': 12,\n",
    "    'dim_head': 128,\n",
    "    'learning_rate': 1e-5,\n",
    "    'is_training': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, settings):\n",
    "        self.window_size = settings['window_size']\n",
    "        self.dim = settings['embedding_dim']\n",
    "        # read from project gutenberg\n",
    "        sents = []\n",
    "        list(map(sents.extend, list(map(gutenberg.sents, gutenberg.fileids()))))\n",
    "        print('\\n{} sentences fetched.'.format(len(sents)))\n",
    "        # load vocabulary file\n",
    "        with open('vocab.json', 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print('\\n{} unique words found in corpus'.format(len(vocab)))\n",
    "        self.word2id = dict((vocab[i], i) for i in range(len(vocab)))\n",
    "        self.data = []\n",
    "        for sent in sents:\n",
    "            for i in range(len(sent)):\n",
    "                try:\n",
    "                    context = [self.word2id[word] for word in sent[max(0, i - self.window_size):i] + sent[i+1:min(\n",
    "                        len(sent), i + 1 + self.window_size)]]\n",
    "                    target = self.word2id[sent[i]]\n",
    "                    while len(context) < 2*self.window_size:\n",
    "                        context.append(0)\n",
    "                    self.data.append((target, context))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        print('\\n{} pairs found for training'.format(self.__len__()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = torch.Tensor([self.data[index][0]])\n",
    "        context = torch.Tensor(self.data[index][1])\n",
    "        return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "98552 sentences fetched.\n",
      "\n",
      "51156 unique words found in corpus\n",
      "\n",
      "2621762 pairs found for training\n"
     ]
    }
   ],
   "source": [
    "dataset = myDataset(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_leng = dataset.__len__() // 10\n",
    "leng = dataset.__len__()\n",
    "train_set, test_set, dev_set = torch.utils.data.random_split(dataset, [uni_leng*8, uni_leng, leng-9*uni_leng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=settings['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=settings['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=settings['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_model(nn.Module):\n",
    "    def __init__(self, settings):\n",
    "        super(w2v_model, self).__init__()\n",
    "        self.vocab_size = settings['vocab_size']\n",
    "        self.batch_size = settings['batch_size']\n",
    "        self.num_heads = settings['num_heads']\n",
    "        self.dim_head = settings['dim_head']\n",
    "        self.num_hidden = self.dim_head * self.num_heads\n",
    "        self.seq_len = settings['window_size'] * 2\n",
    "        self.embed_dim = settings['embedding_dim']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "#         self.embedding = torch.randn([self.vocab_size, self.embed_dim], requires_grad=True)\n",
    "        self.W_Q = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_K = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_V = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def attention(self, target, context):\n",
    "        Q = self.W_Q(target).view(self.batch_size, self.num_heads, self.dim_head)\n",
    "        W = torch.zeros([self.batch_size, self.seq_len, self.num_heads, self.num_heads]).to(target.device)\n",
    "        V = torch.zeros([self.batch_size, self.seq_len, self.num_hidden]).to(target.device)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.seq_len):\n",
    "                K_t = self.W_K(context[i][j]).view(self.num_heads, self.dim_head).transpose(0, 1)\n",
    "                W[i][j] = torch.matmul(Q[i], K_t) / (self.dim_head ** 0.5)\n",
    "                V[i][j] = self.W_V(context[i][j])\n",
    "        W = nn.Softmax(dim=-1)(W)\n",
    "        V = V.view(self.batch_size, self.seq_len, self.num_heads, self.dim_head)\n",
    "        tmp = torch.matmul(W, V).view(self.batch_size, self.seq_len, self.num_hidden)\n",
    "        context_vector = torch.sum(tmp, dim=1).view(self.batch_size, self.num_hidden)\n",
    "        target_vector = self.W_V(target).view(self.batch_size, self.num_hidden)\n",
    "        return target_vector, context_vector\n",
    "    \n",
    "    def forward(self, t, c):\n",
    "        target = self.embedding(t.long())\n",
    "        context = self.embedding(c.long())\n",
    "        v_t, v_c = self.attention(target, context)\n",
    "        return v_t, v_c\n",
    "#         sim = self.cos_sim(v_t, v_c)\n",
    "#         return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and settings['is_training']:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v_model(settings).to(device)\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=settings['learning_rate'], momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "def get_embed(token):\n",
    "    return model.embedding(torch.Tensor([vocab.index(token)]).long().to(device))\n",
    "def most_similar(token, num_return):\n",
    "    v_w1 = get_embed(token)\n",
    "    word_sim = {}\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        v_w2 = get_embed(word)\n",
    "        theta = cos_sim(v_w1, v_w2)\n",
    "        word_sim[word] = theta.detach().numpy()\n",
    "    words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    for word, sim in words_sorted[:num_return]:\n",
    "        yield (word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 loss: 0.000053 time used for 10 steps 42.297522\n",
      "epoch 1 step 0 loss: 0.000052 time used for 10 steps 47.269611\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_steps = train_set.__len__()//settings['batch_size']\n",
    "for epoch in range(settings['num_epochs']):\n",
    "    for step in range(train_set.__len__()//settings['batch_size']):\n",
    "        start = time.time()\n",
    "        (t, c) = next(iter(train_loader))\n",
    "        t, c = t.to(device), c.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        v_t, v_c = model(t, c)\n",
    "        loss = lossfunc(v_t, v_c.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 0:\n",
    "            print('epoch {} step {} loss: {:.6f} time used for 10 steps {:6f}'.format(\n",
    "                epoch, step, loss.tolist(), time.time()-start))\n",
    "            writer.add_scalar('speed', time.time()-start, epoch*num_steps+step)\n",
    "            \n",
    "            model.eval()\n",
    "            (t, c) = next(iter(test_loader))\n",
    "            t, c = t.to(device), c.to(device)\n",
    "            v_t, v_c = model(t, c)\n",
    "            test_loss = lossfunc(v_t, v_c.to(device))\n",
    "            (t, c) = next(iter(dev_loader))\n",
    "            t, c = t.to(device), c.to(device)\n",
    "            v_t, v_c = model(t, c)\n",
    "            dev_loss = lossfunc(v_t, v_c.to(device))\n",
    "            writer.add_scalars('loss', {'train': loss.tolist(),\n",
    "                                        'test': test_loss.tolist(),\n",
    "                                        'dev': dev_loss.tolist()\n",
    "                                       }, epoch*num_steps+step)\n",
    "            model.train()\n",
    "    torch.save(model.state_dict(), 'MSE_SGD/epoch_{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_model_CBoW(nn.Module):\n",
    "    def __init__(self, settings):\n",
    "        super(w2v_model_CBoW, self).__init__()\n",
    "        self.vocab_size = settings['vocab_size']\n",
    "        self.batch_size = settings['batch_size']\n",
    "        self.num_heads = settings['num_heads']\n",
    "        self.dim_head = settings['dim_head']\n",
    "        self.num_hidden = self.dim_head * self.num_heads\n",
    "        self.seq_len = settings['window_size'] * 2\n",
    "        self.embed_dim = settings['embedding_dim']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.W_Q = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_K = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_V = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_out = nn.Linear(self.num_hidden, self.vocab_size)\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def attention(self, target, context):\n",
    "        Q = self.W_Q(target).view(self.batch_size, self.num_heads, self.dim_head)\n",
    "        W = torch.zeros([self.batch_size, self.seq_len, self.num_heads, self.num_heads]).to(target.device)\n",
    "        V = torch.zeros([self.batch_size, self.seq_len, self.num_hidden]).to(target.device)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.seq_len):\n",
    "                K_t = self.W_K(context[i][j]).view(self.num_heads, self.dim_head).transpose(0, 1)\n",
    "                W[i][j] = torch.matmul(Q[i], K_t) / (self.dim_head ** 0.5)\n",
    "                V[i][j] = self.W_V(context[i][j])\n",
    "        W = nn.Softmax(dim=-1)(W)\n",
    "        V = V.view(self.batch_size, self.seq_len, self.num_heads, self.dim_head)\n",
    "        tmp = torch.matmul(W, V).view(self.batch_size, self.seq_len, self.num_hidden)\n",
    "        context_vector = torch.sum(tmp, dim=1).view(self.batch_size, self.num_hidden)\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, t, c):\n",
    "        target = self.embedding(t.long())\n",
    "        context = self.embedding(c.long())\n",
    "        v_c = self.attention(target, context)\n",
    "        pred = nn.Softmax(dim=1)(self.W_out(v_c))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = w2v_model_CBoW(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tmp_model(t,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 51156])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELoss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8427, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CELoss(pred, t.long().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
