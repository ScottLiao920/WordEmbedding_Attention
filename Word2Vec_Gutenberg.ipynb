{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chang.liao/anaconda3/envs/venv/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all sentences in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for sent in gutenberg.sents(file):\n",
    "#         sents.append(sent)\n",
    "# len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model using gensim for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(sents, size=100, window=5, min_count=1, workers=4)\n",
    "# model.save('word2vec_gensim.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary for all the words in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for word in gutenberg.words(file):\n",
    "#         words.append(word)\n",
    "# # words = list(set(words))\n",
    "# len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd = nltk.FreqDist(words)\n",
    "# vocab = sorted(fd, key=fd.get, reverse=True)\n",
    "# vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab', 'wb') as f:\n",
    "#     for i in range(len(vocab)):\n",
    "#         f.write('{} {}\\n'.format(vocab[i], i).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.json', 'w') as f:\n",
    "#     json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since we only have 2.6 million words, Skip-Gram should performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'window_size': 5,\n",
    "    'num_epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'batch_size': 512,\n",
    "    'num_heads': 12,\n",
    "    'dim_head': 128,\n",
    "    'learning_rate': 1e-5,\n",
    "    'is_training': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, settings):\n",
    "        self.window_size = settings['window_size']\n",
    "        self.dim = settings['embedding_dim']\n",
    "        # read from project gutenberg\n",
    "        sents = []\n",
    "        list(map(sents.extend, list(map(gutenberg.sents, gutenberg.fileids()))))\n",
    "        print('\\n{} sentences fetched.'.format(len(sents)))\n",
    "        # load vocabulary file\n",
    "        with open('vocab.json', 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print('\\n{} unique words found in corpus'.format(len(vocab)))\n",
    "        self.word2id = dict((vocab[i], i) for i in range(len(vocab)))\n",
    "        self.data = []\n",
    "        for sent in sents:\n",
    "            for i in range(len(sent)):\n",
    "                try:\n",
    "                    context = [self.word2id[word] for word in sent[max(0, i - self.window_size):i] + sent[i+1:min(\n",
    "                        len(sent), i + 1 + self.window_size)]]\n",
    "                    target = self.word2id[sent[i]]\n",
    "                    while len(context) < 2*self.window_size:\n",
    "                        context.append(0)\n",
    "                    self.data.append((target, context))\n",
    "                except KeyError:\n",
    "                    print(sent[max(0, i - self.window_size):min(len(sent), i + 1 + self.window_size)])\n",
    "        print('{} pairs found for training'.format(self.__len__()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = torch.Tensor([self.data[index][0]])\n",
    "        context = torch.Tensor(self.data[index][1])\n",
    "        return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "98552 sentences fetched.\n",
      "\n",
      "51156 unique words found in corpus\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory', 'shakes']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory', 'shakes', 'by']\n",
      "['\"*']\n",
      "['\"*']\n",
      "['\\'\".']\n",
      "['\"*']\n",
      "['nd', 'desart', 'ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['desart', 'ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "2621762 pairs found for training\n"
     ]
    }
   ],
   "source": [
    "dataset = myDataset(settings)\n",
    "dataloader = DataLoader(dataset, batch_size=settings['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_model(nn.Module):\n",
    "    def __init__(self, settings):\n",
    "        super(w2v_model, self).__init__()\n",
    "        self.vocab_size = settings['vocab_size']\n",
    "        self.batch_size = settings['batch_size']\n",
    "        self.num_heads = settings['num_heads']\n",
    "        self.dim_head = settings['dim_head']\n",
    "        self.num_hidden = self.dim_head * self.num_heads\n",
    "        self.seq_len = settings['window_size'] * 2\n",
    "        self.embed_dim = settings['embedding_dim']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "#         self.embedding = torch.randn([self.vocab_size, self.embed_dim], requires_grad=True)\n",
    "        self.W_Q = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_K = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_V = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def attention(self, target, context):\n",
    "        Q = self.W_Q(target).view(self.batch_size, self.num_heads, self.dim_head)\n",
    "        W = torch.zeros([self.batch_size, self.seq_len, self.num_heads, self.num_heads]).to(target.device)\n",
    "        V = torch.zeros([self.batch_size, self.seq_len, self.num_hidden]).to(target.device)\n",
    "        \n",
    "        # zero-padding\n",
    "        for i in range(self.batch_size):\n",
    "            K_t = self.W_K(target[i]).view(self.num_heads, self.dim_head).transpose(0,1)\n",
    "            for j in range(self.seq_len):\n",
    "                W[i][j] = torch.matmul(Q[i], K_t) / (self.dim_head ** 0.5)\n",
    "                V[i][j] = self.W_V(target[j])\n",
    "        W = nn.Softmax(dim=-1)(W)\n",
    "        V = V.view(self.batch_size, self.seq_len, self.num_heads, self.dim_head)\n",
    "        tmp = torch.matmul(W, V).view(self.batch_size, self.seq_len, self.num_hidden)\n",
    "        context_vector = torch.sum(tmp, dim=1).view(self.batch_size, self.num_hidden)\n",
    "        target_vector = self.W_V(target).view(self.batch_size, self.num_hidden)\n",
    "        return target_vector, context_vector\n",
    "    \n",
    "    def forward(self, t, c):\n",
    "        target = self.embedding(t.long())\n",
    "        context = self.embedding(c.long())\n",
    "        v_t, v_c = self.attention(target, context)\n",
    "        return v_t, v_c\n",
    "#         sim = self.cos_sim(v_t, v_c)\n",
    "#         return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and settings['is_training']:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v_model(settings).to(device)\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=settings['learning_rate'], momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w2v_model(\n",
       "  (embedding): Embedding(51156, 50)\n",
       "  (W_Q): Linear(in_features=50, out_features=1536, bias=True)\n",
       "  (W_K): Linear(in_features=50, out_features=1536, bias=True)\n",
       "  (W_V): Linear(in_features=50, out_features=1536, bias=True)\n",
       "  (cos_sim): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('MSE_SGD/epoch_50.pt', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model = gensim.models.KeyedVectors.load('word2vec_gensim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ladies', 0.778356671333313),\n",
       " ('women', 0.7729319930076599),\n",
       " ('whales', 0.7110983729362488),\n",
       " ('nations', 0.7061511278152466),\n",
       " ('people', 0.698354959487915),\n",
       " ('children', 0.6776918172836304),\n",
       " ('ones', 0.6673187017440796),\n",
       " ('those', 0.6658756732940674),\n",
       " ('cities', 0.6576958298683167),\n",
       " ('heathen', 0.6557672023773193)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model.wv.most_similar(['men'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7288056 , -0.7894503 ,  0.2997309 , -0.634888  ,  1.1730815 ,\n",
       "         1.0118821 ,  1.5643903 ,  0.84447783, -0.03041133, -1.0454463 ,\n",
       "         2.1104195 ,  0.17344235, -1.0437306 , -0.11676285,  0.19619858,\n",
       "         0.69553906,  0.22397263,  0.43834323, -1.1550168 , -1.3721713 ,\n",
       "        -0.2732976 ,  0.63333327,  0.568821  , -0.55983514, -0.09155396,\n",
       "         0.21911587,  1.5157677 , -1.0443634 ,  0.20765564,  0.11520106,\n",
       "         0.0727839 , -0.00374739,  1.6814423 , -1.8273432 , -0.77540773,\n",
       "        -0.9494474 , -1.5475677 ,  0.05887098, -0.36490342,  0.4780317 ,\n",
       "         1.4733663 ,  1.0463972 ,  0.19754106,  0.38320884, -0.9215026 ,\n",
       "        -0.6607583 , -0.22892128,  0.09999726, -0.29728153,  0.0046324 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding(torch.Tensor([0]).long().to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "def get_embed(token):\n",
    "    return model.embedding(torch.Tensor([vocab.index(token)]).long().to(device))\n",
    "def most_similar(token, num_return):\n",
    "    v_w1 = get_embed(token)\n",
    "    word_sim = {}\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        v_w2 = get_embed(word)\n",
    "        theta = cos_sim(v_w1, v_w2)\n",
    "        word_sim[word] = theta.detach().numpy()\n",
    "    words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    for word, sim in words_sorted[:num_return]:\n",
    "        yield (word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0534], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(get_embed('men'),get_embed('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('men', array([1.], dtype=float32)),\n",
       " ('builders', array([0.59341156], dtype=float32)),\n",
       " ('louing', array([0.5396602], dtype=float32)),\n",
       " ('Shibboleth', array([0.52968055], dtype=float32)),\n",
       " ('Hampton', array([0.527706], dtype=float32)),\n",
       " ('Obseruers', array([0.50242555], dtype=float32)),\n",
       " ('savourest', array([0.4790864], dtype=float32)),\n",
       " ('GENEROUS', array([0.47566143], dtype=float32)),\n",
       " ('Redeemer', array([0.4740326], dtype=float32)),\n",
       " ('patchy', array([0.47305432], dtype=float32))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(most_similar('men',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('women', array([1.], dtype=float32)),\n",
       " ('Brazon', array([0.5396867], dtype=float32)),\n",
       " ('nails', array([0.5330049], dtype=float32)),\n",
       " ('overcoming', array([0.52965105], dtype=float32)),\n",
       " ('seate', array([0.5219931], dtype=float32)),\n",
       " ('loungingly', array([0.5204232], dtype=float32)),\n",
       " ('noting', array([0.51352113], dtype=float32)),\n",
       " ('cleared', array([0.5008819], dtype=float32)),\n",
       " ('Conduit', array([0.49569836], dtype=float32)),\n",
       " ('vnprouokes', array([0.4861741], dtype=float32))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(most_similar('women',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 9 loss: 0.971289 time used for 10 steps 26.969929\n",
      "epoch 0 step 19 loss: 0.777529 time used for 10 steps 27.684524\n",
      "epoch 0 step 29 loss: 0.610333 time used for 10 steps 27.541535\n",
      "epoch 0 step 39 loss: 0.718405 time used for 10 steps 27.704566\n",
      "epoch 0 step 49 loss: 0.669113 time used for 10 steps 27.596583\n",
      "epoch 0 step 59 loss: 0.693762 time used for 10 steps 27.376994\n",
      "epoch 0 step 69 loss: 0.769555 time used for 10 steps 27.946891\n",
      "epoch 0 step 79 loss: 0.885623 time used for 10 steps 26.890848\n",
      "epoch 0 step 89 loss: 0.669561 time used for 10 steps 27.529937\n",
      "epoch 0 step 99 loss: 0.717460 time used for 10 steps 27.764776\n",
      "epoch 0 step 109 loss: 0.715162 time used for 10 steps 27.423823\n",
      "epoch 0 step 119 loss: 0.613195 time used for 10 steps 26.284391\n",
      "epoch 0 step 129 loss: 0.732654 time used for 10 steps 25.617761\n",
      "epoch 0 step 139 loss: 0.933855 time used for 10 steps 25.654312\n",
      "epoch 0 step 149 loss: 0.814789 time used for 10 steps 26.032262\n",
      "epoch 0 step 159 loss: 0.721976 time used for 10 steps 26.260000\n",
      "epoch 0 step 169 loss: 0.800315 time used for 10 steps 26.031543\n",
      "epoch 0 step 179 loss: 0.651457 time used for 10 steps 25.840603\n",
      "epoch 0 step 189 loss: 0.954402 time used for 10 steps 25.261743\n",
      "epoch 0 step 199 loss: 0.784690 time used for 10 steps 26.161855\n",
      "epoch 0 step 209 loss: 0.833396 time used for 10 steps 26.271301\n",
      "epoch 0 step 219 loss: 0.698083 time used for 10 steps 25.921275\n",
      "epoch 0 step 229 loss: 0.628218 time used for 10 steps 25.739389\n",
      "epoch 0 step 239 loss: 0.855209 time used for 10 steps 25.293017\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(settings['num_epochs']):\n",
    "    for step in range(dataset.__len__()//settings['batch_size']):\n",
    "        (t, c) = next(iter(dataloader))\n",
    "        t, c = t.to(device), c.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        v_t, v_c = model(t, c)\n",
    "        loss = lossfunc(v_t, v_c.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 9:\n",
    "            print('epoch {} step {} loss: {:.6f} time used for 10 steps {:6f}'.format(\n",
    "                epoch, step, loss.tolist(), time.time()-start))\n",
    "            start = time.time()\n",
    "    torch.save(model.state_dict(), 'MSE_SGD/epoch_{}.pt'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
