{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chang.liao/anaconda3/envs/venv/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all sentences in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for sent in gutenberg.sents(file):\n",
    "#         sents.append(sent)\n",
    "# len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model using gensim for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(sents, size=100, window=5, min_count=1, workers=4)\n",
    "# model.save('word2vec_gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary for all the words in Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# for file in gutenberg.fileids():\n",
    "#     for word in gutenberg.words(file):\n",
    "#         words.append(word)\n",
    "# # words = list(set(words))\n",
    "# len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd = nltk.FreqDist(words)\n",
    "# vocab = sorted(fd, key=fd.get, reverse=True)\n",
    "# vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab', 'wb') as f:\n",
    "#     for i in range(len(vocab)):\n",
    "#         f.write('{} {}\\n'.format(vocab[i], i).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.json', 'w') as f:\n",
    "#     json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since we only have 2.6 million words, Skip-Gram should performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'window_size': 5,\n",
    "    'num_epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'batch_size': 256,\n",
    "    'num_heads': 12,\n",
    "    'dim_head': 64,\n",
    "    'learning_rate': 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, settings):\n",
    "        self.window_size = settings['window_size']\n",
    "        self.dim = settings['embedding_dim']\n",
    "        # read from project gutenberg\n",
    "        sents = []\n",
    "        list(map(sents.extend, list(map(gutenberg.sents, gutenberg.fileids()))))\n",
    "        print('\\n{} sentences fetched.'.format(len(sents)))\n",
    "        # load vocabulary file\n",
    "        with open('vocab.json', 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print('\\n{} unique words found in corpus'.format(len(vocab)))\n",
    "        self.word2id = dict((vocab[i], i) for i in range(len(vocab)))\n",
    "        self.data = []\n",
    "        for sent in sents:\n",
    "            for i in range(len(sent)):\n",
    "                try:\n",
    "                    context = [self.word2id[word] for word in sent[max(0, i - self.window_size):i] + sent[i+1:min(\n",
    "                        len(sent), i + 1 + self.window_size)]]\n",
    "                    target = self.word2id[sent[i]]\n",
    "                    while len(context) < 2*self.window_size:\n",
    "                        context.append(0)\n",
    "                    self.data.append((target, context))\n",
    "                except KeyError:\n",
    "                    print(sent[max(0, i - self.window_size):min(len(sent), i + 1 + self.window_size)])\n",
    "        print('{} pairs found for training'.format(self.__len__()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = torch.Tensor([self.data[index][0]])\n",
    "        context = torch.Tensor(self.data[index][1])\n",
    "        return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "98552 sentences fetched.\n",
      "\n",
      "51156 unique words found in corpus\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*', 'The', 'saying', 'become', 'proverbial', 'in', 'the', 'village', '.']\n",
      "['\"*']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory', 'shakes']\n",
      "['\"*', 'Each', 'who', 'answers', '\"', 'A', 'Talbotite', ',\"', 'Rory', 'shakes', 'by']\n",
      "['\"*']\n",
      "['\"*']\n",
      "['\\'\".']\n",
      "['\"*']\n",
      "['nd', 'desart', 'ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['desart', 'ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['ways', 'with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['with', '?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['?', 'oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "['oeril', 'gone', 'All', '?', 'might', ',?']\n",
      "2621762 pairs found for training\n"
     ]
    }
   ],
   "source": [
    "dataset = myDataset(settings)\n",
    "dataloader = DataLoader(dataset, batch_size=settings['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_model(nn.Module):\n",
    "    def __init__(self, settings):\n",
    "        super(w2v_model, self).__init__()\n",
    "        self.vocab_size = settings['vocab_size']\n",
    "        self.batch_size = settings['batch_size']\n",
    "        self.num_heads = settings['num_heads']\n",
    "        self.dim_head = settings['dim_head']\n",
    "        self.num_hidden = self.dim_head * self.num_heads\n",
    "        self.seq_len = settings['window_size'] * 2\n",
    "        self.embed_dim = settings['embedding_dim']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "#         self.embedding = torch.randn([self.vocab_size, self.embed_dim], requires_grad=True)\n",
    "        self.W_Q = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_K = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.W_V = nn.Linear(self.embed_dim, self.num_hidden)\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def attention(self, target, context):\n",
    "        Q = self.W_Q(target).view(self.batch_size, self.num_heads, self.dim_head)\n",
    "        W = torch.zeros([self.batch_size, self.seq_len, self.num_heads, self.num_heads])\n",
    "        V = torch.zeros([self.batch_size, self.seq_len, self.num_hidden])\n",
    "        \n",
    "        # zero-padding\n",
    "        for i in range(self.batch_size):\n",
    "            K_t = self.W_K(target[i]).view(self.num_heads, self.dim_head).transpose(0,1)\n",
    "            for j in range(self.seq_len):\n",
    "                W[i][j] = torch.matmul(Q[i], K_t) / (self.dim_head ** 0.5)\n",
    "                V[i][j] = self.W_V(target[j])\n",
    "        W = nn.Softmax(dim=-1)(W)\n",
    "        V = V.view(self.batch_size, self.seq_len, self.num_heads, self.dim_head)\n",
    "        \n",
    "        tmp = torch.matmul(W, V).view(self.batch_size, self.seq_len, self.num_hidden)\n",
    "        context_vector = torch.sum(tmp, dim=1)\n",
    "        target_vector = self.W_V(target).view(self.batch_size, self.num_hidden)\n",
    "        return target_vector, context_vector.view(self.batch_size, self.num_hidden)\n",
    "    \n",
    "    def forward(self, t, c):\n",
    "        target = self.embedding(t.long())\n",
    "        context = self.embedding(c.long())\n",
    "        v_t, v_c = self.attention(target, context)\n",
    "        return v_t, v_c\n",
    "#         sim = self.cos_sim(v_t, v_c)\n",
    "#         return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v_model(settings)\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7311)\n"
     ]
    }
   ],
   "source": [
    "for step, (t, c) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    v_t, v_c = model(t, c)\n",
    "    loss = lossfunc(v_t, v_c)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 10 == 0:\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
