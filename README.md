# WordEmbedding_Attention
Skip-Gram word embedding with context vector calculated by attention mechanism

## TODOs:
* Modify model architecture for higher efficiency
* Add GPU support
* Fine tuning
